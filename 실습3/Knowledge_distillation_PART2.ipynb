{"cells":[{"cell_type":"markdown","metadata":{"id":"iAiGIre6n2qT"},"source":["# **1. 가상환경 설정 및 라이브러리 불러오기**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33477,"status":"ok","timestamp":1699802757540,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"pMSTg5eF8V8U","outputId":"98328c8a-4dd3-4c16-ca13-a91568796dc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13508,"status":"ok","timestamp":1699802771044,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"mgcUBh4YExcY","outputId":"380e5302-ce10-4268-953e-ceb115819bc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ptflops\n","  Downloading ptflops-0.7.1.2.tar.gz (15 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.1.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ptflops) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ptflops) (1.3.0)\n","Building wheels for collected packages: ptflops\n","  Building wheel for ptflops (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ptflops: filename=ptflops-0.7.1.2-py3-none-any.whl size=13214 sha256=349386a913b3f626ff50edcf1aaafc0e6091d8776164eaefe780ac832bc0655c\n","  Stored in directory: /root/.cache/pip/wheels/9d/90/07/20e8c3221349a85d63b319593e1bcbb6e0c995d2e2bcc5d775\n","Successfully built ptflops\n","Installing collected packages: ptflops\n","Successfully installed ptflops-0.7.1.2\n"]}],"source":["import os\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import random_split\n","!pip install ptflops\n","from ptflops import get_model_complexity_info\n","\n","import random\n","from scipy.stats import norm\n","import scipy\n","import math\n","import time\n","\n","try:\n","    from torch.hub import load_state_dict_from_url\n","except ImportError:\n","    from torch.utils.model_zoo import load_url as load_state_dict_from_url"]},{"cell_type":"code","source":["# device : gpu를 사용할 경우에는 'cuda', 그렇지 않을 경우에는 'cpu'\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","# 랜덤 시드 고정\n","# 실험 조건을 동일하게 설정하여 같은 input을 넣으면 같은 결과가 나올 수 있도록 함\n","random.seed(777)\n","torch.manual_seed(777)\n","if device == 'cuda':\n","  torch.cuda.manual_seed_all(777)"],"metadata":{"id":"eyXuOwhtJtQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12447,"status":"ok","timestamp":1699802783479,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"7woqU9-ZF8CV","outputId":"009f81ff-e661-49b1-efab-70831bfc1618"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:05<00:00, 29522252.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","45000\n","5000\n","10000\n","True\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["# load the data\n","train_transform = transforms.Compose(\n","    [\n","     transforms.RandomCrop(32, padding=4),\n","     transforms.RandomHorizontalFlip(),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784))])\n","\n","test_transform = transforms.Compose(\n","    [\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784))])\n","\n","ds = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n","test_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n","\n","# split the training set and validation set\n","torch.manual_seed(50)\n","test_size = len(test_ds)\n","val_size = 5000\n","train_size = len(ds) - val_size\n","batch_size = 256\n","\n","train_ds, val_ds = random_split(ds, [train_size, val_size])\n","\n","train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n","val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n","test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","classes = ds.classes\n","\n","print(train_size)\n","print(val_size)\n","print(test_size)\n","print(torch.cuda.is_available())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8715,"status":"ok","timestamp":1699802792187,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"8XSSmFIZORJ0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"700b9db3-6bff-462a-ba6f-84795ee63152"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":5}],"source":["__all__ = ['ResNet', 'resnet18']\n","\n","def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(BasicBlock, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError(\n","                'BasicBlock only supports groups=1 and base_width=64')\n","        if dilation > 1:\n","            raise NotImplementedError(\n","                \"Dilation > 1 not supported in BasicBlock\")\n","        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n","    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n","    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n","    # This variant is also known as ResNet V1.5 and improves accuracy according to\n","    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n","\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(Bottleneck, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        width = int(planes * (base_width / 64.)) * groups\n","        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv1x1(inplanes, width)\n","        self.bn1 = norm_layer(width)\n","        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n","        self.bn2 = norm_layer(width)\n","        self.conv3 = conv1x1(width, planes * self.expansion)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","\n","    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n","                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n","                 norm_layer=None):\n","        super(ResNet, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            # each element in the tuple indicates if we should replace\n","            # the 2x2 stride with a dilated convolution instead\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\"replace_stride_with_dilation should be None \"\n","                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1,\n","                            bias=False)\n","\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n","                                       dilate=replace_stride_with_dilation[0])\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n","                                       dilate=replace_stride_with_dilation[1])\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n","                                       dilate=replace_stride_with_dilation[2])\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(\n","                    m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n","        norm_layer = self._norm_layer\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                norm_layer(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n","                            self.base_width, previous_dilation, norm_layer))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups=self.groups,\n","                                base_width=self.base_width, dilation=self.dilation,\n","                                norm_layer=norm_layer))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _forward_impl(self, x):\n","        # See note [TorchScript super()]\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def forward(self, x):\n","        return self._forward_impl(x)\n","\n","    def get_bn_before_relu(self):\n","        if isinstance(self.layer1[0], Bottleneck):\n","            bn1 = self.layer1[-1].bn3\n","            bn2 = self.layer2[-1].bn3\n","            bn3 = self.layer3[-1].bn3\n","            bn4 = self.layer4[-1].bn3\n","        elif isinstance(self.layer1[0], BasicBlock):\n","            bn1 = self.layer1[-1].bn2\n","            bn2 = self.layer2[-1].bn2\n","            bn3 = self.layer3[-1].bn2\n","            bn4 = self.layer4[-1].bn2\n","        else:\n","            print('ResNet unknown block error !!!')\n","\n","        return [bn1, bn2, bn3, bn4]\n","\n","    def get_channel_num(self):\n","        return [64,128,256,512]\n","\n","    def extract_feature(self, x):\n","\n","      x = self.conv1(x)\n","      x = self.bn1(x)\n","      x = self.relu(x)\n","      x = self.maxpool(x)\n","\n","      feat1 = self.layer1(x)\n","      feat2 = self.layer2(feat1)\n","      feat3 = self.layer3(feat2)\n","      feat4 = self.layer4(feat3)\n","\n","      x = self.avgpool(feat4)\n","      x = torch.flatten(x, 1)\n","      x = self.fc(x)\n","\n","      return [feat1,feat2,feat3,feat4],x\n","\n","model_teacher = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=10)\n","\n","model_teacher.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/checkpoints/original_best.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tip_XdAYFGRK"},"outputs":[],"source":["__all__ = ['ResNet', 'resnet18']\n","\n","def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=dilation, groups=in_planes, bias=False, dilation=dilation)\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","class BasicBlock_light(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(BasicBlock_light, self).__init__()\n","\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError(\n","                'BasicBlock only supports groups=1 and base_width=64')\n","        if dilation > 1:\n","            raise NotImplementedError(\n","                \"Dilation > 1 not supported in BasicBlock\")\n","        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n","        self.dwconv33_1 = conv3x3(inplanes, planes, stride)\n","        self.conv11_1 = conv1x1(planes, planes)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.dwconv33_2 = conv3x3(planes, planes)\n","        self.conv11_2 = conv1x1(planes, planes)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.dwconv33_1(x)\n","        out = self.conv11_1(out)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.dwconv33_2(out)\n","        out = self.conv11_2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck_light(nn.Module):\n","    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n","    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n","    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n","    # This variant is also known as ResNet V1.5 and improves accuracy according to\n","    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n","\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(Bottleneck_light, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        width = int(planes * (base_width / 64.)) * groups\n","        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv1x1(inplanes, width)\n","        self.bn1 = norm_layer(width)\n","        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n","        self.bn2 = norm_layer(width)\n","        self.conv3 = conv1x1(width, planes * self.expansion)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet_lightweight(nn.Module):\n","\n","    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n","                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n","                 norm_layer=None):\n","        super(ResNet_lightweight, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            # each element in the tuple indicates if we should replace\n","            # the 2x2 stride with a dilated convolution instead\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\"replace_stride_with_dilation should be None \"\n","                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n","                                       dilate=replace_stride_with_dilation[0])\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n","                                       dilate=replace_stride_with_dilation[1])\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n","                                       dilate=replace_stride_with_dilation[2])\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(\n","                    m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck_light):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock_light):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n","        norm_layer = self._norm_layer\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                norm_layer(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n","                            self.base_width, previous_dilation, norm_layer))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups=self.groups,\n","                                base_width=self.base_width, dilation=self.dilation,\n","                                norm_layer=norm_layer))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _forward_impl(self, x):\n","        # See note [TorchScript super()]\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def forward(self, x):\n","        return self._forward_impl(x)\n","\n","    def get_bn_before_relu(self):\n","        if isinstance(self.layer1[0], Bottleneck):\n","            bn1 = self.layer1[-1].bn3\n","            bn2 = self.layer2[-1].bn3\n","            bn3 = self.layer3[-1].bn3\n","            bn4 = self.layer4[-1].bn3\n","        elif isinstance(self.layer1[0], BasicBlock):\n","            bn1 = self.layer1[-1].bn2\n","            bn2 = self.layer2[-1].bn2\n","            bn3 = self.layer3[-1].bn2\n","            bn4 = self.layer4[-1].bn2\n","        else:\n","            print('ResNet unknown block error !!!')\n","\n","        return [bn1, bn2, bn3, bn4]\n","\n","    def get_channel_num(self):\n","        return [64,128,256,512]\n","\n","    def extract_feature(self, x):\n","      x = self.conv1(x)\n","      x = self.bn1(x)\n","      x = self.relu(x)\n","      x = self.maxpool(x)\n","\n","      feat1 = self.layer1(x)\n","      feat2 = self.layer2(feat1)\n","      feat3 = self.layer3(feat2)\n","      feat4 = self.layer4(feat3)\n","\n","      x = self.avgpool(feat4)\n","      x = torch.flatten(x, 1)\n","      x = self.fc(x)\n","\n","      return [feat1,feat2,feat3,feat4], x\n","\n","model_student = ResNet_lightweight(block=BasicBlock_light, layers=[2, 2, 2, 2], num_classes=10)"]},{"cell_type":"code","source":["def distillation_loss(source, target, margin):\n","    target = torch.max(target, margin)\n","    loss = torch.nn.functional.mse_loss(source, target, reduction=\"none\")\n","    loss = loss * ((source > target) | (target > 0)).float()\n","    return loss.sum()\n","\n","def build_feature_connector(t_channel, s_channel):\n","    C = [nn.Conv2d(s_channel, t_channel, kernel_size=1, stride=1, padding=0, bias=False),\n","         nn.BatchNorm2d(t_channel)]\n","\n","    for m in C:\n","        if isinstance(m, nn.Conv2d):\n","            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            m.weight.data.normal_(0, math.sqrt(2. / n))\n","        elif isinstance(m, nn.BatchNorm2d):\n","            m.weight.data.fill_(1)\n","            m.bias.data.zero_()\n","\n","    return nn.Sequential(*C)\n","\n","def get_margin_from_BN(bn):\n","    margin = []\n","    std = bn.weight.data\n","    mean = bn.bias.data\n","    for (s, m) in zip(std, mean):\n","        s = abs(s.item())\n","        m = m.item()\n","        if norm.cdf(-m / s) > 0.001:\n","            margin.append(- s * math.exp(- (m / s) ** 2 / 2) / math.sqrt(2 * math.pi) / norm.cdf(-m / s) + m)\n","        else:\n","            margin.append(-3 * s)\n","\n","    return torch.FloatTensor(margin).to(std.device)\n","\n","class Distiller(nn.Module):\n","    def __init__(self, t_net, s_net):\n","        super(Distiller, self).__init__()\n","\n","        t_channels = t_net.get_channel_num()\n","        s_channels = s_net.get_channel_num()\n","\n","        self.Connectors = nn.ModuleList([build_feature_connector(t, s) for t, s in zip(t_channels, s_channels)])\n","\n","        teacher_bns = t_net.get_bn_before_relu()\n","        margins = [get_margin_from_BN(bn) for bn in teacher_bns]\n","        for i, margin in enumerate(margins):\n","            self.register_buffer('margin%d' % (i+1), margin.unsqueeze(1).unsqueeze(2).unsqueeze(0).detach())\n","\n","        self.t_net = t_net\n","        self.s_net = s_net\n","\n","    def forward(self, x):\n","\n","        t_feats, t_out = self.t_net.extract_feature(x)\n","        s_feats, s_out = self.s_net.extract_feature(x)\n","        feat_num = len(t_feats)\n","\n","        loss_distill = 0\n","        for i in range(feat_num):\n","            s_feats[i] = self.Connectors[i](s_feats[i])\n","            loss_distill += distillation_loss(s_feats[i], t_feats[i].detach(), getattr(self, 'margin%d' % (i+1)))/ 2 ** (feat_num - i - 1)\n","\n","        return s_out, loss_distill"],"metadata":{"id":"pfq6r4eeWTyO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Module for distillation\n","d_net = Distiller(model_teacher, model_student)"],"metadata":{"id":"NkVN4cZAnW9C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate(model, criterion, val_loader, use_gpu=False):\n","  val_size = len(val_loader.dataset)\n","  val_loss = 0\n","  correct = 0\n","  device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n","\n","  with torch.no_grad():\n","    for i, data in enumerate(val_loader):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs).to(device)\n","        loss = criterion(outputs, labels)\n","\n","        val_loss += loss * inputs.size(0)\n","\n","        # val accuracy\n","        _, predicted = torch.max(outputs.data, 1)\n","        correct += (predicted == labels).sum().item()\n","\n","    val_loss = val_loss/val_size\n","    val_accuracy = correct/val_size;\n","\n","  return val_loss, val_accuracy\n","\n","def get_train_accuracy(model, criterion, train_loader, use_gpu=False):\n","   train_size = len(train_loader.dataset)\n","   correct = 0\n","   device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n","\n","   with torch.no_grad():\n","    for i, data in enumerate(train_loader):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs).to(device)\n","\n","        # val accuracy\n","        _, predicted = torch.max(outputs.data, 1)\n","        correct += (predicted == labels).sum().item()\n","\n","    print(correct)\n","    return correct/train_size\n","\n","def train_with_distill(d_net, criterion, optimizer, train_loader, val_loader, epoch, use_gpu=False):\n","  best_loss = 99999\n","  train_size = len(train_loader.dataset)\n","  device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n","\n","  history = {\n","      'train_loss': [],\n","      'train_accuracy': [],\n","      'val_loss': [],\n","      'val_accuracy': []\n","      }\n","\n","\n","  d_net = d_net.to(device)\n","\n","  d_net.train()\n","  d_net.s_net.train()\n","  d_net.t_net.train()\n","\n","  for epoch in range(epoch):  # loop over the dataset multiple times\n","    running_loss = 0.0\n","    correct = 0\n","\n","    print(f'------------------------------\\n Epoch: {epoch + 1}')\n","\n","    t1 = time.time()\n","    for i,data in enumerate(train_loader):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        batch_size = inputs.shape[0]\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        outputs, loss_distill = d_net(inputs)\n","        loss_CE = criterion(outputs, labels)\n","        loss = loss_CE + loss_distill.sum() / batch_size / 1000\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # running_loss += loss.item() * inputs.size(0)\n","        running_loss += loss_CE.item()\n","    t2 = time.time()\n","    t = t2 - t1\n","\n","    # save the model\n","    PATH = './drive/My Drive/Colab Notebooks/checkpoints/'\n","\n","    if loss < best_loss:\n","      best_loss = loss\n","      torch.save(d_net.s_net.state_dict(), os.path.join(PATH, \"feature_distillation_student_best.pth\"))\n","\n","    epoch_train_loss = running_loss/train_size\n","    epoch_train_accuracy = get_train_accuracy(d_net.s_net, criterion, train_loader, use_gpu)\n","    epoch_val_loss, epoch_val_accuracy = validate(d_net.s_net, criterion, val_loader, use_gpu)\n","    print(f'time: {int(t)}sec train_loss: {epoch_train_loss}, train_accuracy: {epoch_train_accuracy}, val_loss: {epoch_val_loss}, val_accuracy: {epoch_val_accuracy}');\n","\n","    history['train_loss'].append(epoch_train_loss)\n","    history['train_accuracy'].append(epoch_train_accuracy)\n","    history['val_loss'].append(epoch_val_loss)\n","    history['val_accuracy'].append(epoch_val_accuracy)\n","\n","  return history"],"metadata":{"id":"7yP9kQQbua6F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model_student.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n","\n","history = train_with_distill(d_net, criterion, optimizer, train_loader, val_loader, 10, torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JL_Xld261Gid","executionInfo":{"status":"ok","timestamp":1699803729359,"user_tz":-540,"elapsed":459693,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"}},"outputId":"bed8bd31-7a49-4688-f335-0c43b59dd0ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------\n"," Epoch: 1\n","31436\n","time: 22sec train_loss: 0.0034563106020291646, train_accuracy: 0.6985777777777777, val_loss: 0.8609227538108826, val_accuracy: 0.6902\n","------------------------------\n"," Epoch: 2\n","31610\n","time: 22sec train_loss: 0.0033145340747303435, train_accuracy: 0.7024444444444444, val_loss: 0.8756552338600159, val_accuracy: 0.6852\n","------------------------------\n"," Epoch: 3\n","32186\n","time: 24sec train_loss: 0.0032002982007132636, train_accuracy: 0.7152444444444445, val_loss: 0.8199408054351807, val_accuracy: 0.7146\n","------------------------------\n"," Epoch: 4\n","32885\n","time: 22sec train_loss: 0.0031169258051448397, train_accuracy: 0.7307777777777777, val_loss: 0.7884931564331055, val_accuracy: 0.7252\n","------------------------------\n"," Epoch: 5\n","32792\n","time: 24sec train_loss: 0.003050018901295132, train_accuracy: 0.7287111111111111, val_loss: 0.7902547121047974, val_accuracy: 0.7142\n","------------------------------\n"," Epoch: 6\n","33447\n","time: 23sec train_loss: 0.0029427363554636638, train_accuracy: 0.7432666666666666, val_loss: 0.7763110399246216, val_accuracy: 0.7298\n","------------------------------\n"," Epoch: 7\n","33495\n","time: 24sec train_loss: 0.002906795361306932, train_accuracy: 0.7443333333333333, val_loss: 0.7500675320625305, val_accuracy: 0.7336\n","------------------------------\n"," Epoch: 8\n","33420\n","time: 22sec train_loss: 0.002848176884651184, train_accuracy: 0.7426666666666667, val_loss: 0.754623532295227, val_accuracy: 0.7332\n","------------------------------\n"," Epoch: 9\n","33596\n","time: 24sec train_loss: 0.0028295235448413426, train_accuracy: 0.7465777777777778, val_loss: 0.75080406665802, val_accuracy: 0.735\n","------------------------------\n"," Epoch: 10\n","34261\n","time: 22sec train_loss: 0.0027668777293629117, train_accuracy: 0.7613555555555556, val_loss: 0.7190900444984436, val_accuracy: 0.7478\n"]}]},{"cell_type":"code","source":["def test(model, criterion, test_loader, use_gpu=False):\n","  test_size = len(test_loader.dataset)\n","  device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n","  test_loss = 0.0\n","  test_accuracy = 0\n","  correct = 0\n","  model.eval()\n","  with torch.no_grad():\n","      for i, data in enumerate(test_loader):\n","          # get the inputs; data is a list of [inputs, labels]\n","          inputs, labels = data[0].to(device), data[1].to(device)\n","\n","          # forward + backward + optimize\n","          outputs = model(inputs).to(device)\n","          loss = criterion(outputs, labels)\n","\n","          test_loss += loss * inputs.size(0)\n","\n","          # val accuracy\n","          _, predicted = torch.max(outputs.data, 1)\n","          correct += (predicted == labels).sum().item()\n","\n","\n","      test_loss = test_loss/test_size\n","      test_accuracy = correct/test_size\n","\n","  return test_loss, test_accuracy"],"metadata":{"id":"Hm6TO21tVFkX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loss, test_accuracy = test(model_student, criterion, test_loader, True)"],"metadata":{"id":"GeKglTQcVHk1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(test_loss,test_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OWd9XtKDVI_M","executionInfo":{"status":"ok","timestamp":1699803788876,"user_tz":-540,"elapsed":11,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"}},"outputId":"7e51add9-80bc-41dd-e267-3da7dcefe9d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.6835, device='cuda:0') 0.7618\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1699803788876,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"VEMmlMB8NALZ","outputId":"b5930f6a-2d1a-4ce8-ce4f-fd49f2ebe117"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: module BasicBlock_light is treated as a zero-op.\n","Warning: module ResNet_lightweight is treated as a zero-op.\n","ResNet_lightweight(\n","  1.62 M, 100.000% Params, 21.89 MMac, 98.844% MACs, \n","  (conv1): Conv2d(1.73 k, 0.107% Params, 1.77 MMac, 7.989% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(128, 0.008% Params, 131.07 KMac, 0.592% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(0, 0.000% Params, 65.54 KMac, 0.296% MACs, inplace=True)\n","  (maxpool): MaxPool2d(0, 0.000% Params, 65.54 KMac, 0.296% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    19.2 k, 1.188% Params, 4.98 MMac, 22.488% MACs, \n","    (0): BasicBlock_light(\n","      9.6 k, 0.594% Params, 2.49 MMac, 11.244% MACs, \n","      (dwconv33_1): Conv2d(576, 0.036% Params, 147.46 KMac, 0.666% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","      (conv11_1): Conv2d(4.1 k, 0.254% Params, 1.05 MMac, 4.734% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, 0.008% Params, 32.77 KMac, 0.148% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 32.77 KMac, 0.148% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(576, 0.036% Params, 147.46 KMac, 0.666% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","      (conv11_2): Conv2d(4.1 k, 0.254% Params, 1.05 MMac, 4.734% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, 0.008% Params, 32.77 KMac, 0.148% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock_light(\n","      9.6 k, 0.594% Params, 2.49 MMac, 11.244% MACs, \n","      (dwconv33_1): Conv2d(576, 0.036% Params, 147.46 KMac, 0.666% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","      (conv11_1): Conv2d(4.1 k, 0.254% Params, 1.05 MMac, 4.734% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, 0.008% Params, 32.77 KMac, 0.148% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 32.77 KMac, 0.148% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(576, 0.036% Params, 147.46 KMac, 0.666% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","      (conv11_2): Conv2d(4.1 k, 0.254% Params, 1.05 MMac, 4.734% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, 0.008% Params, 32.77 KMac, 0.148% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    79.62 k, 4.928% Params, 5.13 MMac, 23.154% MACs, \n","    (0): BasicBlock_light(\n","      44.03 k, 2.725% Params, 2.83 MMac, 12.798% MACs, \n","      (dwconv33_1): Conv2d(1.15 k, 0.071% Params, 73.73 KMac, 0.333% MACs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n","      (conv11_1): Conv2d(16.38 k, 1.014% Params, 1.05 MMac, 4.734% MACs, 128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, 0.016% Params, 16.38 KMac, 0.074% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 16.38 KMac, 0.074% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(1.15 k, 0.071% Params, 73.73 KMac, 0.333% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","      (conv11_2): Conv2d(16.38 k, 1.014% Params, 1.05 MMac, 4.734% MACs, 128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, 0.016% Params, 16.38 KMac, 0.074% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        8.45 k, 0.523% Params, 540.67 KMac, 2.441% MACs, \n","        (0): Conv2d(8.19 k, 0.507% Params, 524.29 KMac, 2.367% MACs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, 0.016% Params, 16.38 KMac, 0.074% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock_light(\n","      35.58 k, 2.202% Params, 2.29 MMac, 10.356% MACs, \n","      (dwconv33_1): Conv2d(1.15 k, 0.071% Params, 73.73 KMac, 0.333% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","      (conv11_1): Conv2d(16.38 k, 1.014% Params, 1.05 MMac, 4.734% MACs, 128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, 0.016% Params, 16.38 KMac, 0.074% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 16.38 KMac, 0.074% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(1.15 k, 0.071% Params, 73.73 KMac, 0.333% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","      (conv11_2): Conv2d(16.38 k, 1.014% Params, 1.05 MMac, 4.734% MACs, 128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, 0.016% Params, 16.38 KMac, 0.074% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    306.69 k, 18.982% Params, 4.92 MMac, 22.229% MACs, \n","    (0): BasicBlock_light(\n","      169.98 k, 10.521% Params, 2.73 MMac, 12.317% MACs, \n","      (dwconv33_1): Conv2d(2.3 k, 0.143% Params, 36.86 KMac, 0.166% MACs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n","      (conv11_1): Conv2d(65.54 k, 4.056% Params, 1.05 MMac, 4.734% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, 0.032% Params, 8.19 KMac, 0.037% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 8.19 KMac, 0.037% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(2.3 k, 0.143% Params, 36.86 KMac, 0.166% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n","      (conv11_2): Conv2d(65.54 k, 4.056% Params, 1.05 MMac, 4.734% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, 0.032% Params, 8.19 KMac, 0.037% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        33.28 k, 2.060% Params, 532.48 KMac, 2.404% MACs, \n","        (0): Conv2d(32.77 k, 2.028% Params, 524.29 KMac, 2.367% MACs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, 0.032% Params, 8.19 KMac, 0.037% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock_light(\n","      136.7 k, 8.461% Params, 2.2 MMac, 9.913% MACs, \n","      (dwconv33_1): Conv2d(2.3 k, 0.143% Params, 36.86 KMac, 0.166% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n","      (conv11_1): Conv2d(65.54 k, 4.056% Params, 1.05 MMac, 4.734% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, 0.032% Params, 8.19 KMac, 0.037% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 8.19 KMac, 0.037% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(2.3 k, 0.143% Params, 36.86 KMac, 0.166% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n","      (conv11_2): Conv2d(65.54 k, 4.056% Params, 1.05 MMac, 4.734% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, 0.032% Params, 8.19 KMac, 0.037% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    1.2 M, 74.470% Params, 4.82 MMac, 21.767% MACs, \n","    (0): BasicBlock_light(\n","      667.65 k, 41.323% Params, 2.67 MMac, 12.076% MACs, \n","      (dwconv33_1): Conv2d(4.61 k, 0.285% Params, 18.43 KMac, 0.083% MACs, 256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n","      (conv11_1): Conv2d(262.14 k, 16.225% Params, 1.05 MMac, 4.734% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1.02 k, 0.063% Params, 4.1 KMac, 0.018% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 4.1 KMac, 0.018% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(4.61 k, 0.285% Params, 18.43 KMac, 0.083% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n","      (conv11_2): Conv2d(262.14 k, 16.225% Params, 1.05 MMac, 4.734% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(1.02 k, 0.063% Params, 4.1 KMac, 0.018% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        132.1 k, 8.176% Params, 528.38 KMac, 2.386% MACs, \n","        (0): Conv2d(131.07 k, 8.112% Params, 524.29 KMac, 2.367% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1.02 k, 0.063% Params, 4.1 KMac, 0.018% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock_light(\n","      535.55 k, 33.147% Params, 2.15 MMac, 9.691% MACs, \n","      (dwconv33_1): Conv2d(4.61 k, 0.285% Params, 18.43 KMac, 0.083% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n","      (conv11_1): Conv2d(262.14 k, 16.225% Params, 1.05 MMac, 4.734% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1.02 k, 0.063% Params, 4.1 KMac, 0.018% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 4.1 KMac, 0.018% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(4.61 k, 0.285% Params, 18.43 KMac, 0.083% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n","      (conv11_2): Conv2d(262.14 k, 16.225% Params, 1.05 MMac, 4.734% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(1.02 k, 0.063% Params, 4.1 KMac, 0.018% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 2.05 KMac, 0.009% MACs, output_size=(1, 1))\n","  (fc): Linear(5.13 k, 0.318% Params, 5.13 KMac, 0.023% MACs, in_features=512, out_features=10, bias=True)\n",")\n","22.15 MMac macs\n","1.62 M params\n"]}],"source":["net = model_student\n","macs, params = get_model_complexity_info(net, (3, 32, 32), as_strings=True,\n","                                           print_per_layer_stat=True, verbose=True)\n","print(macs,'macs')\n","print(params,'params')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1OFcPBY7gxCsi0o27CAarlyggD7N42lu9","timestamp":1698061340102}]},"kernelspec":{"display_name":"aa","language":"python","name":"time2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}