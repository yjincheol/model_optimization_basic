{"cells":[{"cell_type":"markdown","metadata":{"id":"iAiGIre6n2qT"},"source":["# **1. 가상환경 설정 및 라이브러리 불러오기**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27423,"status":"ok","timestamp":1699015233183,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"mgcUBh4YExcY","outputId":"7c16e276-415e-47a2-9533-7b14bb758357"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnx\n","  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n","Installing collected packages: onnx\n","Successfully installed onnx-1.15.0\n","Collecting ptflops\n","  Downloading ptflops-0.7.1.2.tar.gz (15 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.1.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ptflops) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ptflops) (1.3.0)\n","Building wheels for collected packages: ptflops\n","  Building wheel for ptflops (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ptflops: filename=ptflops-0.7.1.2-py3-none-any.whl size=13213 sha256=11fada89c1b600ce694af776c20f55d4d5db477ceb4031193d0d125574f90e3a\n","  Stored in directory: /root/.cache/pip/wheels/9d/90/07/20e8c3221349a85d63b319593e1bcbb6e0c995d2e2bcc5d775\n","Successfully built ptflops\n","Installing collected packages: ptflops\n","Successfully installed ptflops-0.7.1.2\n"]}],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import random_split\n","!pip install onnx\n","!pip install ptflops\n","from ptflops import get_model_complexity_info"]},{"cell_type":"code","source":["import random\n","# device : gpu를 사용할 경우에는 'cuda', 그렇지 않을 경우에는 'cpu'\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","# 랜덤 시드 고정\n","# 실험 조건을 동일하게 설정하여 같은 input을 넣으면 같은 결과가 나올 수 있도록 함\n","random.seed(777)\n","torch.manual_seed(777)\n","if device == 'cuda':\n","  torch.cuda.manual_seed_all(777)"],"metadata":{"id":"BmqG6AZh-S6W","executionInfo":{"status":"ok","timestamp":1699015233955,"user_tz":-540,"elapsed":774,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23409,"status":"ok","timestamp":1699015257356,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"7woqU9-ZF8CV","outputId":"27eb66fd-39ed-40ab-dc2e-cfde1819be18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:19<00:00, 8870189.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","45000\n","5000\n","10000\n","True\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["# load the data\n","train_transform = transforms.Compose(\n","    [\n","     transforms.RandomCrop(32, padding=4),\n","     transforms.RandomHorizontalFlip(),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784))])\n","\n","test_transform = transforms.Compose(\n","    [\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784))])\n","\n","ds = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=train_transform)\n","\n","\n","test_ds = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=test_transform)\n","\n","# split the training set and validation set\n","torch.manual_seed(50)\n","test_size = len(test_ds)\n","val_size = 5000\n","train_size = len(ds) - val_size\n","batch_size = 256\n","\n","train_ds, val_ds = random_split(ds, [train_size, val_size])\n","\n","\n","train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","classes = ds.classes\n","\n","print(train_size)\n","print(val_size)\n","print(test_size)\n","print(torch.cuda.is_available())"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25323,"status":"ok","timestamp":1699015282665,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"pMSTg5eF8V8U","outputId":"bb502428-2ec3-48a2-a49d-23815ac7653f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8456,"status":"ok","timestamp":1699015291116,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"8XSSmFIZORJ0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"91b6a846-c8bf-48c6-d615-d0008dc18d8b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":5}],"source":["import torch\n","import torch.nn as nn\n","\n","try:\n","    from torch.hub import load_state_dict_from_url\n","except ImportError:\n","    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n","\n","\n","__all__ = ['ResNet', 'resnet18']\n","\n","\n","model_urls = {\n","    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n","}\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(BasicBlock, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError(\n","                'BasicBlock only supports groups=1 and base_width=64')\n","        if dilation > 1:\n","            raise NotImplementedError(\n","                \"Dilation > 1 not supported in BasicBlock\")\n","        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n","    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n","    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n","    # This variant is also known as ResNet V1.5 and improves accuracy according to\n","    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n","\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(Bottleneck, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        width = int(planes * (base_width / 64.)) * groups\n","        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv1x1(inplanes, width)\n","        self.bn1 = norm_layer(width)\n","        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n","        self.bn2 = norm_layer(width)\n","        self.conv3 = conv1x1(width, planes * self.expansion)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","\n","    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n","                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n","                 norm_layer=None):\n","        super(ResNet, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            # each element in the tuple indicates if we should replace\n","            # the 2x2 stride with a dilated convolution instead\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\"replace_stride_with_dilation should be None \"\n","                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1,\n","                            bias=False)\n","\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n","                                       dilate=replace_stride_with_dilation[0])\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n","                                       dilate=replace_stride_with_dilation[1])\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n","                                       dilate=replace_stride_with_dilation[2])\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(\n","                    m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n","        norm_layer = self._norm_layer\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                norm_layer(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n","                            self.base_width, previous_dilation, norm_layer))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups=self.groups,\n","                                base_width=self.base_width, dilation=self.dilation,\n","                                norm_layer=norm_layer))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _forward_impl(self, x):\n","        # See note [TorchScript super()]\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def forward(self, x):\n","        return self._forward_impl(x)\n","\n","# Teacher model build\n","model_teacher = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=10)\n","\n","# Teacher model checkpoint 적용\n","model_teacher.load_state_dict(torch.load('/content/drive/MyDrive/teacher-e9.pth'))"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1699015291116,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"tip_XdAYFGRK"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","try:\n","    from torch.hub import load_state_dict_from_url\n","except ImportError:\n","    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n","\n","\n","__all__ = ['ResNet', 'resnet18']\n","\n","\n","model_urls = {\n","    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n","}\n","\n","def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=dilation, groups=in_planes, bias=False, dilation=dilation)\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","class BasicBlock_light(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(BasicBlock_light, self).__init__()\n","\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError(\n","                'BasicBlock only supports groups=1 and base_width=64')\n","        if dilation > 1:\n","            raise NotImplementedError(\n","                \"Dilation > 1 not supported in BasicBlock\")\n","        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n","        self.dwconv33_1 = conv3x3(inplanes, planes, stride)\n","        self.conv11_1 = conv1x1(planes, planes)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.dwconv33_2 = conv3x3(planes, planes)\n","        self.conv11_2 = conv1x1(planes, planes)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.dwconv33_1(x)\n","        out = self.conv11_1(out)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.dwconv33_2(out)\n","        out = self.conv11_2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","class Bottleneck_light(nn.Module):\n","    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n","    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n","    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n","    # This variant is also known as ResNet V1.5 and improves accuracy according to\n","    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n","\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n","                 base_width=64, dilation=1, norm_layer=None):\n","        super(Bottleneck_light, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        width = int(planes * (base_width / 64.)) * groups\n","        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv1x1(inplanes, width)\n","        self.bn1 = norm_layer(width)\n","        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n","        self.bn2 = norm_layer(width)\n","        self.conv3 = conv1x1(width, planes * self.expansion)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","class ResNet_lightweight(nn.Module):\n","\n","    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n","                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n","                 norm_layer=None):\n","        super(ResNet_lightweight, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            # each element in the tuple indicates if we should replace\n","            # the 2x2 stride with a dilated convolution instead\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\"replace_stride_with_dilation should be None \"\n","                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n","                            bias=False)\n","\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n","                                       dilate=replace_stride_with_dilation[0])\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n","                                       dilate=replace_stride_with_dilation[1])\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n","                                       dilate=replace_stride_with_dilation[2])\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(\n","                    m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck_light):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock_light):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n","        norm_layer = self._norm_layer\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                norm_layer(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n","                            self.base_width, previous_dilation, norm_layer))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups=self.groups,\n","                                base_width=self.base_width, dilation=self.dilation,\n","                                norm_layer=norm_layer))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _forward_impl(self, x):\n","        # See note [TorchScript super()]\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def forward(self, x):\n","        return self._forward_impl(x)\n","\n","# Student model build\n","model_student = ResNet_lightweight(block=BasicBlock_light, layers=[2, 2, 2, 2], num_classes=10)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1699015291116,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"T5dB4kF7Gdva"},"outputs":[],"source":["import time\n","\n","def validate(model, criterion, val_loader, use_gpu=False):\n","  val_size = len(val_loader.dataset)\n","  val_loss = 0\n","  correct = 0\n","  device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n","\n","  with torch.no_grad():\n","    for i, data in enumerate(val_loader):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs).to(device)\n","        loss = criterion(outputs, labels)\n","\n","        val_loss += loss * inputs.size(0)\n","\n","        # val accuracy\n","        _, predicted = torch.max(outputs.data, 1)\n","        correct += (predicted == labels).sum().item()\n","\n","    val_loss = val_loss/val_size\n","    val_accuracy = correct/val_size;\n","\n","  return val_loss, val_accuracy\n","\n","def get_train_accuracy(model, criterion, train_loader, use_gpu=False):\n","   train_size = len(train_loader.dataset)\n","   correct = 0\n","   device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n","\n","   with torch.no_grad():\n","    for i, data in enumerate(train_loader):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs).to(device)\n","\n","        # val accuracy\n","        _, predicted = torch.max(outputs.data, 1)\n","        correct += (predicted == labels).sum().item()\n","\n","    print(correct)\n","    return correct/train_size\n","\n","def train(model_student,model_teacher, criterion, criterion_kd, optimizer, train_loader, val_loader, epoch=2, use_gpu=False):\n","\n","  train_size = len(train_loader.dataset)\n","  device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n","\n","  history = {\n","      'train_loss': [],\n","      'train_accuracy': [],\n","      'val_loss': [],\n","      'val_accuracy': []\n","      }\n","\n","  model_student = model_student.to(device)\n","  model_teacher = model_teacher.to(device)\n","\n","  for epoch in range(epoch):  # loop over the dataset multiple times\n","    running_loss = 0.0\n","    correct = 0\n","\n","    print(f'------------------------------\\n Epoch: {epoch + 1}')\n","\n","    t1 = time.time()\n","    for  i,data in enumerate(train_loader):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs_student = model_student(inputs)\n","        outputs_teacher = model_teacher(inputs)\n","\n","        loss_label = criterion(outputs_student, labels)\n","        loss_kd = criterion_kd(outputs_student,outputs_teacher)\n","\n","\n","        loss = loss_label + loss_kd\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # as loss.item() return the average batch loss, so convert it to the total loss\n","        running_loss += loss.item() * inputs.size(0)\n","\n","    t2 = time.time()\n","    t = t2 - t1\n","\n","    # save the model\n","    # PATH = f'./drive/My Drive/Colab Notebooks/checkpoints/resnet18-original-e{epoch}.pth'\n","    PATH = f'./drive/My Drive/checkpoints/resnet18-kd-e{epoch}.pth'\n","    if epoch % 5 == 0:\n","      torch.save({'epoch': epoch, 'state_dict': model_student.state_dict(), 'optimizer': optimizer.state_dict(),}, PATH)\n","\n","    epoch_train_loss = running_loss/train_size\n","    epoch_train_accuracy = get_train_accuracy(model_student, criterion, train_loader, use_gpu)\n","    epoch_val_loss, epoch_val_accuracy = validate(model_student, criterion, val_loader, use_gpu)\n","    print(f'time: {int(t)}sec train_loss: {epoch_train_loss}, train_accuracy: {epoch_train_accuracy}, val_loss: {epoch_val_loss}, val_accuracy: {epoch_val_accuracy}');\n","\n","    history['train_loss'].append(epoch_train_loss)\n","    history['train_accuracy'].append(epoch_train_accuracy)\n","    history['val_loss'].append(epoch_val_loss)\n","    history['val_accuracy'].append(epoch_val_accuracy)\n","\n","  return history\n","\n","def train_student_only(model, criterion, optimizer, train_loader, val_loader, epoch=2, use_gpu=False):\n","\n","  train_size = len(train_loader.dataset)\n","  device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n","\n","  history = {\n","      'train_loss': [],\n","      'train_accuracy': [],\n","      'val_loss': [],\n","      'val_accuracy': []\n","      }\n","\n","  model = model.to(device)\n","\n","  for epoch in range(epoch):  # loop over the dataset multiple times\n","    running_loss = 0.0\n","    correct = 0\n","\n","    print(f'------------------------------\\n Epoch: {epoch + 1}')\n","\n","    t1 = time.time()\n","    for  i,data in enumerate(train_loader):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # as loss.item() return the average batch loss, so convert it to the total loss\n","        running_loss += loss.item() * inputs.size(0)\n","\n","    t2 = time.time()\n","    t = t2 - t1\n","\n","    # save the model\n","    PATH = f'./drive/My Drive/checkpoints/resnet18-student-e{epoch}.pth'\n","    # if epoch+1 % 5 == 0:\n","\n","    torch.save(model.state_dict(), PATH)\n","    # torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),}, PATH)\n","\n","    epoch_train_loss = running_loss/train_size\n","    epoch_train_accuracy = get_train_accuracy(model, criterion, train_loader, use_gpu)\n","    epoch_val_loss, epoch_val_accuracy = validate(model, criterion, val_loader, use_gpu)\n","    print(f'time: {int(t)}sec train_loss: {epoch_train_loss}, train_accuracy: {epoch_train_accuracy}, val_loss: {epoch_val_loss}, val_accuracy: {epoch_val_accuracy}');\n","\n","    history['train_loss'].append(epoch_train_loss)\n","    history['train_accuracy'].append(epoch_train_accuracy)\n","    history['val_loss'].append(epoch_val_loss)\n","    history['val_accuracy'].append(epoch_val_accuracy)\n","\n","  return history"]},{"cell_type":"code","source":["optimizer = optim.SGD(model_student.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","def CriterionKD(pred,soft, temperature = 4):\n","    '''\n","    knowledge distillation loss\n","    '''\n","    criterion_kd = torch.nn.KLDivLoss()\n","    scale_pred = pred\n","    scale_soft = soft\n","    loss = criterion_kd(F.log_softmax(scale_pred / temperature, dim=1), F.softmax(scale_soft / temperature, dim=1))\n","\n","    return loss * temperature * temperature"],"metadata":{"id":"D8x87Lj6734w","executionInfo":{"status":"ok","timestamp":1699015291116,"user_tz":-540,"elapsed":12,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":482017,"status":"ok","timestamp":1699015773121,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"zCBeSMOeGiAT","outputId":"9539c344-2281-44ee-d068-e5e5c5ae6aa2"},"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------\n"," Epoch: 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["22913\n","time: 33sec train_loss: 2.012724509684245, train_accuracy: 0.5091777777777777, val_loss: 1.3486446142196655, val_accuracy: 0.5032\n","------------------------------\n"," Epoch: 2\n","26489\n","time: 27sec train_loss: 1.4749772007836235, train_accuracy: 0.5886444444444444, val_loss: 1.1714941263198853, val_accuracy: 0.5852\n","------------------------------\n"," Epoch: 3\n","28754\n","time: 26sec train_loss: 1.2617930751588609, train_accuracy: 0.6389777777777778, val_loss: 1.0291457176208496, val_accuracy: 0.6358\n","------------------------------\n"," Epoch: 4\n","30503\n","time: 26sec train_loss: 1.0978274841414557, train_accuracy: 0.6778444444444445, val_loss: 0.930529773235321, val_accuracy: 0.6648\n","------------------------------\n"," Epoch: 5\n","30943\n","time: 26sec train_loss: 1.0135901771333482, train_accuracy: 0.6876222222222222, val_loss: 0.9037443399429321, val_accuracy: 0.6748\n","------------------------------\n"," Epoch: 6\n","32372\n","time: 26sec train_loss: 0.9305613579856025, train_accuracy: 0.7193777777777778, val_loss: 0.8230656981468201, val_accuracy: 0.7126\n","------------------------------\n"," Epoch: 7\n","33330\n","time: 24sec train_loss: 0.8716804425981309, train_accuracy: 0.7406666666666667, val_loss: 0.7816508412361145, val_accuracy: 0.7246\n","------------------------------\n"," Epoch: 8\n","33736\n","time: 25sec train_loss: 0.8221049434343973, train_accuracy: 0.7496888888888888, val_loss: 0.7356559634208679, val_accuracy: 0.7418\n","------------------------------\n"," Epoch: 9\n","34169\n","time: 24sec train_loss: 0.7835974402003818, train_accuracy: 0.7593111111111112, val_loss: 0.7337857484817505, val_accuracy: 0.7354\n","------------------------------\n"," Epoch: 10\n","34893\n","time: 26sec train_loss: 0.7516797021336026, train_accuracy: 0.7754, val_loss: 0.6792832612991333, val_accuracy: 0.7638\n"]}],"source":["history = train(model_student,model_teacher, criterion, CriterionKD, optimizer, train_loader, val_loader, 10, torch.cuda.is_available())\n","#history = train_student_only(model_student, criterion, optimizer, train_loader, val_loader, 10, torch.cuda.is_available())"]},{"cell_type":"code","source":["def test(model, criterion, test_loader, use_gpu=False):\n","  test_size = len(test_loader.dataset)\n","  device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n","  test_loss = 0.0\n","  test_accuracy = 0\n","  correct = 0\n","  model.eval()\n","  with torch.no_grad():\n","      for i, data in enumerate(test_loader):\n","          # get the inputs; data is a list of [inputs, labels]\n","          inputs, labels = data[0].to(device), data[1].to(device)\n","\n","          # forward + backward + optimize\n","          outputs = model(inputs).to(device)\n","          loss = criterion(outputs, labels)\n","\n","          test_loss += loss * inputs.size(0)\n","\n","          # val accuracy\n","          _, predicted = torch.max(outputs.data, 1)\n","          correct += (predicted == labels).sum().item()\n","\n","\n","      test_loss = test_loss/test_size\n","      test_accuracy = correct/test_size\n","\n","  return test_loss, test_accuracy"],"metadata":{"id":"Sa9dHDDMLixP","executionInfo":{"status":"ok","timestamp":1699015773121,"user_tz":-540,"elapsed":13,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["test_loss, test_accuracy = test(model_student, criterion, test_loader, True)\n","history['test_loss'] = test_loss\n","history['test_accuracy'] = test_accuracy"],"metadata":{"id":"0KBnbfRQLkrC","executionInfo":{"status":"ok","timestamp":1699015942199,"user_tz":-540,"elapsed":2569,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["print(history['test_loss'], history['test_accuracy'] )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FNRlY55ULznx","executionInfo":{"status":"ok","timestamp":1699015947159,"user_tz":-540,"elapsed":541,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"}},"outputId":"e3a6b7ba-18c1-4bcd-8f5b-8f96b7bc8691"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.6666, device='cuda:0') 0.7738\n"]}]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":528,"status":"ok","timestamp":1699015953726,"user":{"displayName":"양진철전자공학과","userId":"00438325333115314187"},"user_tz":-540},"id":"VEMmlMB8NALZ","outputId":"8d2ac2b6-342e-4566-954e-8f17ffcf2528"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: module BasicBlock_light is treated as a zero-op.\n","Warning: module ResNet_lightweight is treated as a zero-op.\n","ResNet_lightweight(\n","  1.62 M, 100.000% Params, 21.89 MMac, 98.844% MACs, \n","  (conv1): Conv2d(1.73 k, 0.107% Params, 1.77 MMac, 7.989% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(128, 0.008% Params, 131.07 KMac, 0.592% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(0, 0.000% Params, 65.54 KMac, 0.296% MACs, inplace=True)\n","  (maxpool): MaxPool2d(0, 0.000% Params, 65.54 KMac, 0.296% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    19.2 k, 1.188% Params, 4.98 MMac, 22.488% MACs, \n","    (0): BasicBlock_light(\n","      9.6 k, 0.594% Params, 2.49 MMac, 11.244% MACs, \n","      (dwconv33_1): Conv2d(576, 0.036% Params, 147.46 KMac, 0.666% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","      (conv11_1): Conv2d(4.1 k, 0.254% Params, 1.05 MMac, 4.734% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, 0.008% Params, 32.77 KMac, 0.148% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 32.77 KMac, 0.148% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(576, 0.036% Params, 147.46 KMac, 0.666% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","      (conv11_2): Conv2d(4.1 k, 0.254% Params, 1.05 MMac, 4.734% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, 0.008% Params, 32.77 KMac, 0.148% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock_light(\n","      9.6 k, 0.594% Params, 2.49 MMac, 11.244% MACs, \n","      (dwconv33_1): Conv2d(576, 0.036% Params, 147.46 KMac, 0.666% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","      (conv11_1): Conv2d(4.1 k, 0.254% Params, 1.05 MMac, 4.734% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, 0.008% Params, 32.77 KMac, 0.148% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 32.77 KMac, 0.148% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(576, 0.036% Params, 147.46 KMac, 0.666% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n","      (conv11_2): Conv2d(4.1 k, 0.254% Params, 1.05 MMac, 4.734% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, 0.008% Params, 32.77 KMac, 0.148% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    79.62 k, 4.928% Params, 5.13 MMac, 23.154% MACs, \n","    (0): BasicBlock_light(\n","      44.03 k, 2.725% Params, 2.83 MMac, 12.798% MACs, \n","      (dwconv33_1): Conv2d(1.15 k, 0.071% Params, 73.73 KMac, 0.333% MACs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n","      (conv11_1): Conv2d(16.38 k, 1.014% Params, 1.05 MMac, 4.734% MACs, 128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, 0.016% Params, 16.38 KMac, 0.074% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 16.38 KMac, 0.074% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(1.15 k, 0.071% Params, 73.73 KMac, 0.333% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","      (conv11_2): Conv2d(16.38 k, 1.014% Params, 1.05 MMac, 4.734% MACs, 128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, 0.016% Params, 16.38 KMac, 0.074% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        8.45 k, 0.523% Params, 540.67 KMac, 2.441% MACs, \n","        (0): Conv2d(8.19 k, 0.507% Params, 524.29 KMac, 2.367% MACs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, 0.016% Params, 16.38 KMac, 0.074% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock_light(\n","      35.58 k, 2.202% Params, 2.29 MMac, 10.356% MACs, \n","      (dwconv33_1): Conv2d(1.15 k, 0.071% Params, 73.73 KMac, 0.333% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","      (conv11_1): Conv2d(16.38 k, 1.014% Params, 1.05 MMac, 4.734% MACs, 128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, 0.016% Params, 16.38 KMac, 0.074% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 16.38 KMac, 0.074% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(1.15 k, 0.071% Params, 73.73 KMac, 0.333% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n","      (conv11_2): Conv2d(16.38 k, 1.014% Params, 1.05 MMac, 4.734% MACs, 128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, 0.016% Params, 16.38 KMac, 0.074% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    306.69 k, 18.982% Params, 4.92 MMac, 22.229% MACs, \n","    (0): BasicBlock_light(\n","      169.98 k, 10.521% Params, 2.73 MMac, 12.317% MACs, \n","      (dwconv33_1): Conv2d(2.3 k, 0.143% Params, 36.86 KMac, 0.166% MACs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n","      (conv11_1): Conv2d(65.54 k, 4.056% Params, 1.05 MMac, 4.734% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, 0.032% Params, 8.19 KMac, 0.037% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 8.19 KMac, 0.037% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(2.3 k, 0.143% Params, 36.86 KMac, 0.166% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n","      (conv11_2): Conv2d(65.54 k, 4.056% Params, 1.05 MMac, 4.734% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, 0.032% Params, 8.19 KMac, 0.037% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        33.28 k, 2.060% Params, 532.48 KMac, 2.404% MACs, \n","        (0): Conv2d(32.77 k, 2.028% Params, 524.29 KMac, 2.367% MACs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, 0.032% Params, 8.19 KMac, 0.037% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock_light(\n","      136.7 k, 8.461% Params, 2.2 MMac, 9.913% MACs, \n","      (dwconv33_1): Conv2d(2.3 k, 0.143% Params, 36.86 KMac, 0.166% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n","      (conv11_1): Conv2d(65.54 k, 4.056% Params, 1.05 MMac, 4.734% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, 0.032% Params, 8.19 KMac, 0.037% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 8.19 KMac, 0.037% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(2.3 k, 0.143% Params, 36.86 KMac, 0.166% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n","      (conv11_2): Conv2d(65.54 k, 4.056% Params, 1.05 MMac, 4.734% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, 0.032% Params, 8.19 KMac, 0.037% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    1.2 M, 74.470% Params, 4.82 MMac, 21.767% MACs, \n","    (0): BasicBlock_light(\n","      667.65 k, 41.323% Params, 2.67 MMac, 12.076% MACs, \n","      (dwconv33_1): Conv2d(4.61 k, 0.285% Params, 18.43 KMac, 0.083% MACs, 256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n","      (conv11_1): Conv2d(262.14 k, 16.225% Params, 1.05 MMac, 4.734% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1.02 k, 0.063% Params, 4.1 KMac, 0.018% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 4.1 KMac, 0.018% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(4.61 k, 0.285% Params, 18.43 KMac, 0.083% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n","      (conv11_2): Conv2d(262.14 k, 16.225% Params, 1.05 MMac, 4.734% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(1.02 k, 0.063% Params, 4.1 KMac, 0.018% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        132.1 k, 8.176% Params, 528.38 KMac, 2.386% MACs, \n","        (0): Conv2d(131.07 k, 8.112% Params, 524.29 KMac, 2.367% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1.02 k, 0.063% Params, 4.1 KMac, 0.018% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock_light(\n","      535.55 k, 33.147% Params, 2.15 MMac, 9.691% MACs, \n","      (dwconv33_1): Conv2d(4.61 k, 0.285% Params, 18.43 KMac, 0.083% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n","      (conv11_1): Conv2d(262.14 k, 16.225% Params, 1.05 MMac, 4.734% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1.02 k, 0.063% Params, 4.1 KMac, 0.018% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(0, 0.000% Params, 4.1 KMac, 0.018% MACs, inplace=True)\n","      (dwconv33_2): Conv2d(4.61 k, 0.285% Params, 18.43 KMac, 0.083% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n","      (conv11_2): Conv2d(262.14 k, 16.225% Params, 1.05 MMac, 4.734% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(1.02 k, 0.063% Params, 4.1 KMac, 0.018% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 2.05 KMac, 0.009% MACs, output_size=(1, 1))\n","  (fc): Linear(5.13 k, 0.318% Params, 5.13 KMac, 0.023% MACs, in_features=512, out_features=10, bias=True)\n",")\n","22.15 MMac macs\n","1.62 M params\n"]}],"source":["net = model_student\n","macs, params = get_model_complexity_info(net, (3, 32, 32), as_strings=True,\n","                                           print_per_layer_stat=True, verbose=True)\n","print(macs,'macs')\n","print(params,'params')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1OFcPBY7gxCsi0o27CAarlyggD7N42lu9","timestamp":1698061340102}]},"kernelspec":{"display_name":"aa","language":"python","name":"time2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}